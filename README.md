## Programación Paralela

Repositorio dedicado a las prácticas de la asignatura de Programación Paralela
cursada en el curso 2018-2019 en el Grado de Ingeniería Informática de la UGR.

### Práctica 1 -  Programación en tarjetas gráficas CUDA

 CUDA son las siglas de Compute Unified Device Architecture (Arquitectura
 Unificada de Dispositivos de Cómputo) que hace referencia a una plataforma
 de computación en paralelo incluyendo un compilador y un conjunto de
 herramientas de desarrollo creadas por NVIDIA que permiten a los programadores
 usar una variación del lenguaje de programación C para codificar algoritmos en
 GPU de NVIDIA.

 La prácticas consisten en la paralelización de algoritmos secuenciales mediante
 el entorno de programación CUDA C/C++.

### Práctica 2 - Paralelización de algoritmos con OpenMP y MPI

 OpenMP es una interfaz de programación de aplicaciones (API) para la programación
 multiproceso de memoria compartida en múltiples plataformas. Permite añadir
 concurrencia a los programas escritos en C, C++ y Fortran sobre la base del
 modelo de ejecución fork-join. Está disponible en muchas arquitecturas,
 incluidas las plataformas de Unix y de Microsoft Windows. Se compone de un
 conjunto de directivas de compilador, rutinas de biblioteca, y variables de
 entorno que influyen el comportamiento en tiempo de ejecución.

 MPI ("Message Passing Interface", Interfaz de Paso de Mensajes) es un estándar
 que define la sintaxis y la semántica de las funciones contenidas en una
 biblioteca de paso de mensajes diseñada para ser usada en programas que
 exploten la existencia de múltiples procesadores.

### Práctica 3 - Algoritmo de balanceado de carga con MPI
